\chapter{Discussion}
\label{Discussion}

This chapter first addresses most of the RQ1, specifically if it is feasible to efficiently reimplement the functionality of the desktop tool for learning computing principles by using web technologies.
Then, it continues by addressing RQ2 on the usability and efficiency improvements before going back to device support improvements covered under RQ1.
It finishes with the limitations and ideas for further research.

\section{Feasibility}

As can be seen in \Cref{Design-Implementation}, it was not only possible to reimplement Existing Software using web technologies, but it was also possible to do so relatively efficiently while mitigating the design disadvantages of the web.

Although the implementation involved topics that one may not expect to be well supported on the web, e.g., parsing and code editing, it was shown that there are high-quality \gls{oss} libraries that can support an efficient web implementation.
Notably, the library behind the most popular \gls{ide} is available as a web library\footnote{Monaco Editor, see \Cref{sec:ui-interactions}, powering the popular \gls{ide} Visual Studio Code, see \Cref{sec:writing-code}.} under a permissive \gls{oss} license and was made part of Proposed Software with relative ease.
Similarly, it was possible to create two parsers---for the \gls{hdl} and \gls{tst}\footnote{See Appendix~\ref{appendix:hdl-railroad}, \ref{appendix:hdl-grammar}, \ref{appendix:tst-railroad}, and \ref{appendix:tst-grammar}.}---relatively quickly thanks to an easy-to-use Earley algorithm parser generator and the integrated lexer.\footnote{Nearley and moo, respectively, see \Cref{sec:ui-error-handling}.}
The ability to perform the same selected functionality as Existing Software was confirmed using a range of automated tests, including unit tests demonstrating that all built-in and Project 1 files from Existing Software can be parsed and interpreted.\footnote{\gls{hdl} and \gls{tst} files, see \Cref{sec:evaluation-unit-tests}.}
While it is impossible to say others (re)implementing learning tools on the web would find the ecosystem similarly mature, this work shows it could be worthwhile to evaluate that option.

A reasonable person might expect that web software, compared to desktop, requires an internet connection, and cannot be integrated into the \gls{os} the way desktop applications can.
This thesis shows it is possible to use modern web \glspl{api} to enable use with limited or even no internet connection and install the application like a desktop one.\footnote{Using service workers and \gls{pwa} technology, see \Cref{sec:architecture}.}
Similarly, some might argue that a web application is dependent on servers and harder to reproduce.
Proposed Software is a client-side web application and does not rely on the cloud, which makes it easier to reproduce and enables bundling as a fully functional offline desktop or mobile application.\footnote{Using technology like Electron and Capacitor, see \Cref{sec:architecture}.}
Moreover, we theorise that web-based learning tools could be easier to incorporate into various learning materials using embedding or linking.
This thesis shows that common drawbacks associated with web applications can be solved or mitigated with architectural decisions and modern \glspl{api}.

It was not possible to implement all the functionality offered by Existing Software in the allocated time.
This means there is a possibility some functionality could be harder to cover using the web.
However, the author believes all observed features can be implemented on the web.
Additionally, it should be feasible to do so without complicating the \gls{ui} as it can adapt to the complexity of the solved problems by "growing" with the learner's knowledge on the topic and using "sane defaults".

\section{Improvements}

The presented results show tangible improvements compared to Existing Software regarding usability (especially efficiency) and device support.

As was shown in \Cref{sec:evaluation-efficiency}, participants using Proposed Software embedded within the learning content were able to complete the assigned tasks in one-third of the time ($512 \pm190 s$ vs $1497 \pm326 s$, $\alpha=0.05$, $p<0.001$) it took participants using Existing Software.
It is assumed this effect was partially caused by the overhead of using three different windows: a browser with the learning content, a text editor for the assignment code, and the actual learning tool.
This is also explained by the mean number of times participants from each group got confused,\footnote{See \Cref{sec:methodology-dependent-variables} for the definition.} which was $0.58 \pm 0.42$ for the group using Proposed Software and $2.83 \pm 0.85$ for the group using Existing Software ($\alpha=0.05$, $p<0.001$).
The first set of commonly experienced problems was the inability to run the software due to a missing or incorrect version of Java or the launching of the application using a way unsuitable for their \gls{os}.
The second set of commonly experienced problems was connected to the received errors: participants were often confused by them or could not see them at all since the \gls{ui} would not scale with the window size.
The third set of commonly experienced problems related to the actions available in the user interface---participants were unsure which button to use or how to proceed.
Importantly, this was despite the participants seeing a video showing how to install and use the software.
These problems were non-existent with Proposed Software; the only problem experienced was confusion around the syntax.
This finding suggests it is possible to achieve substantial efficiency gains by reimagining existing desktop learning tools as embedded web learning tools focused on usability.
We could theorise that less time needed to participate and fewer technical challenges would positively impact the \gls{mooc} dropout rates.\footnote{Suggested by research, see \Cref{sec:learning-state}.}
However, further research is needed to confirm that efficiency improvement is reproducible with different learning tools from different fields of study.
This thesis did not cover practical significance, e.g., to what extent this would impact the success rate or dropout rates.

The second metric used to gauge the improvement was the \gls{sus} score mentioned in \Cref{sec:evaluation-questionnaire}.
While there was a difference of 7 points in favour of Proposed Software (86.67 points [97th percentile] vs 79.58 points [86th percentile]), the result was not statistically significant ($\alpha=0.1$, $p=0.22$).
This was likely due to the relatively small effect size and small sample size ($n=12$ per group).
The suggested sample size\footnote{Using SUS Guide \& Calculator Package, see \Cref{sec:methodology-data-analysis}.} at this effect size to produce statistically significant data is $n=45$ per group.
The effect size grew considerably once the same participant was asked to try the other software---likely because of the anchoring.
However, this data was not presented due to the likely bias, as many participants recognised which software was created by the author once they saw both.
Still, it is possible a within-subject design would produce statistically significant data faster, provided the bias could be prevented---e.g., by using a blind data collector.

The data suggest a relation between these two metrics---the time on task and the \gls{sus} score---at $r=-0.28$ and $r=-0.23$ for the group using Proposed Software and Existing Software, respectively.
However, the combination of the small effect size and a small sample size means the likelihood that the null hypothesis is true is high, with $p > 0.35$ for both groups.
More research is needed to confirm this relation in a larger sample size and different works.

Existing research suggests a pattern of increased reliance on devices that do not support running arbitrary desktop applications---particularly among the 18--29 age group.\footnote{See \Cref{sec:device-types}.}
This pattern, combined with the increasing mobile device share on the internet traffic and a new generation of learners used to Chromebooks and tablets, suggests learners could be underserved with existing desktop learning tools.
The problem of reliance on smart devices for learning is particularly apparent in developing countries where about 34\% of households have no access to a desktop, laptop, or tablet device, and one-fourth of the population relies on smartphones as the only way to connect to the internet.\footnote{See \Cref{sec:device-types}, Developing Countries.}
This problem was further emphasised during the COVID pandemic when researchers called for the content to be available from a wide range of devices.\footnote{See \Cref{sec:learning-state}.}
We can argue it is possible to considerably improve the inclusivity of the learning environment by designing for a wide range of devices, which is not the case with legacy desktop learning tools but can be achieved with new web learning tools.
Proposed Software should be available to the vast majority of internet users (estimated 98\%), while only 44\% of the internet traffic is from the desktop, suggesting a large portion of the users would not be able to use the software how they consume web content.
In developing countries, this could mean one-fourth of the population to one-third of the households would be able to get access to learning tools they currently find hard to access or simply cannot access.
However, in developed countries, this may not have a very strong practical impact as the majority can access a compatible laptop or desktop.

Regarding qualitative data, learners who tried both voiced a slight or strong preference for Proposed Software.
They praised the integration within the web page, the integration of the editor, and the automatic execution of tests.
Many noted they felt more efficient, and some noted they would not be willing to install Java if it was not necessary---particularly those on Linux where it was necessary to change the configuration.
Notably, three participants could not finish their session: one did not have the administrative privileges required to install Java, one did not feel comfortable installing software from the internet on the given device, and one could not see the error output as their screen was not large enough and Existing Software does not scale with the window size.
However, it is unclear to what extent the opinions were biased as many of the learners were able to guess which software the author moderating the study wrote once they saw both, and it is assumed learners from developed countries would find a way to use the existing software if they were strongly motivated.

\section{Limitations and Further Work}

This thesis used the case of one specific desktop learning tool.
As such, apart from the certain device support benefits, it is questionable to what extent the same results could be replicated on different learning tools from different fields of study.
Also, it measured the results only on a subset of the functionality and during a session of only one hour.
While the observed efficiency improvements were statistically significant and with a notable effect size, the practical significance will depend on the context.
The results represent a situation where the learner must pick up a new learning tool.
Essential metrics like the success rate or the dropout rate will likely depend heavily on the time limits---be it the time dedicated to the exercise in the classroom or the free time the learner is willing to spend at home before they give up or have to move on.
This thesis did not cover performance improvements connected with a more comprehensive long-term use.
While we could theorise that some of the presented concepts apply to long-term use as well, the effect size would likely get smaller over time.

Moreover, the chosen methodology had several limitations.
Firstly, sampling bias, as the sample came primarily from the extended social circle of the author and only included participants who agreed to participate.
Secondly, moderator bias, as the author was the moderator of the study and might have influenced the study despite the best efforts not to do so.
Lastly, the group using Existing Software had to create the starting files themselves, which could have led to an unrealistic overhead.
While we could argue that this is an expected part of the work with desktop tools, we could also argue that this could have been simplified, and the participants could have been able to download pre-created files.
Using existing files was not possible in this case as the study featured the original bundle of Existing Software and the chosen simple assignments which were not part of the original bundle.
A more extensive comparative study with participants available for longer could use the bundled set of assignments.

The suspected selection bias was shown to hold only for education and occupation with a moderate to strong correlation with performance.\footnote{Higher performance here means lower time taken to prepare and perform tasks, hence the negative correlation coefficient.}
Education data had a correlation coefficient $r(11) = -0.27, p = 0.39$ for Group A and $r(11) = -0.69, p = 0.01$ for Group B.
Occupation data had a correlation coefficient $r(11) = -0.55, p = 0.06$ for Group A and $r(11) = -0.43, p = 0.17$ for Group B.
Simply put, participants from Group B using Existing software had more relevant education and work experience, which seems to correlate positively with better performance.
Therefore, it is possible performance was skewed in favour of Group B using Existing Software.
That being said, if we assume possible moderator bias and the theoretical task overhead due to the file management, the skew was likely minimal and has no impact on the conclusion.

Considering the limitations and unexplored paths of this thesis, there are multiple exciting possibilities for further work:

\begin{itemize}
    \item \textbf{Academic}:
    \begin{itemize}
        \item Larger sample size---suggested $n=45$ per group to achieve statistical significance on all data points.
        \item Long-term study observing performance over several sessions.
        \item Change of the methodology to within-subject with a blind study moderator.
        \item Reproduction of the results with different learning tools from different fields of study.
    \end{itemize}
    \item \textbf{Practical}: Finish implementating all functionality needed to take the whole Nand2Tetris course while using the web implementation.
    \item \textbf{For the Wider Public}: Extend the potential target group further by adapting or introducing content that would use Proposed Software but would be easier to consume by the public.\footnote{Akin to how \url{https://www.elementsofai.com} introduces the basics of AI.}
\end{itemize}
